{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chelsk5/UW-LING-573-Group-5/blob/main/LING573_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Authors: Nina Koh, Chelsea Kendrick\n",
        "# Install dependencies\n",
        "# !pip install --quiet --upgrade pip\n",
        "# !pip install --quiet --upgrade huggingface_hub\n",
        "# !pip install --quiet torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --upgrade\n",
        "# !pip install --quiet --upgrade torchao\n",
        "# !pip install --quiet --upgrade accelerate\n",
        "# !pip install --quiet --upgrade transformers\n",
        "# !pip install --quiet sentence-transformers==2.2.2 fsspec==2023.6.0 numpy==1.26.4\n",
        "# !pip install --quiet --upgrade transformers huggingface_hub sentence-transformers fsspec numpy rouge-metric nltk\n",
        "\n",
        "# !pip install torch==2.0.1+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install torch==2.1.2+cu118 torchvision==0.16.2+cu118 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install --quiet sentence-transformers==2.2.2 transformers==4.30.2 \\\n",
        "#  fsspec==2023.6.0 numpy==1.26.4 numba>=0.60.0\n",
        "# !pip install --upgrade transformers huggingface-hub\n",
        "\n",
        "# !pip install torch==2.0.1 transformers==4.37.2 accelerate==0.25.0 huggingface-hub==0.20.3\n",
        "\n",
        "!pip uninstall -y transformers accelerate huggingface-hub torch torchvision torchaudio \\\n",
        "  tokenizers sentence-transformers peft fastai timm diffusers\n",
        "!pip install torch==2.1.2+cu118 torchvision==0.16.2+cu118 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers==4.38.2 accelerate==0.25.0 huggingface-hub==0.27.1\n",
        "!pip install datasets rouge-metric nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Don't use requirements.txt unless working in a local environment\n",
        "# !pip install -r UW_LING_573_Group_5/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QKo643cp2ut",
        "outputId": "b20d7dc0-59d7-421c-bd48-73684c3147f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.38.2\n",
            "Uninstalling transformers-4.38.2:\n",
            "  Successfully uninstalled transformers-4.38.2\n",
            "Found existing installation: accelerate 0.27.2\n",
            "Uninstalling accelerate-0.27.2:\n",
            "  Successfully uninstalled accelerate-0.27.2\n",
            "Found existing installation: huggingface-hub 0.27.1\n",
            "Uninstalling huggingface-hub-0.27.1:\n",
            "  Successfully uninstalled huggingface-hub-0.27.1\n",
            "Found existing installation: torch 2.1.2+cu118\n",
            "Uninstalling torch-2.1.2+cu118:\n",
            "  Successfully uninstalled torch-2.1.2+cu118\n",
            "Found existing installation: torchvision 0.16.2+cu118\n",
            "Uninstalling torchvision-0.16.2+cu118:\n",
            "  Successfully uninstalled torchvision-0.16.2+cu118\n",
            "Found existing installation: torchaudio 2.1.2+cu118\n",
            "Uninstalling torchaudio-2.1.2+cu118:\n",
            "  Successfully uninstalled torchaudio-2.1.2+cu118\n",
            "Found existing installation: tokenizers 0.15.2\n",
            "Uninstalling tokenizers-0.15.2:\n",
            "  Successfully uninstalled tokenizers-0.15.2\n",
            "\u001b[33mWARNING: Skipping sentence-transformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping peft as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping fastai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping timm as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping diffusers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.1.2+cu118\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.1.2%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n",
            "Collecting torchvision==0.16.2+cu118\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.16.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "Collecting torchaudio==2.1.2\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.1.2%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2+cu118) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2+cu118) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2+cu118) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2+cu118) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2+cu118) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2+cu118) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2+cu118) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.2+cu118) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.2+cu118) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.2+cu118) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.2+cu118) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2+cu118) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2+cu118) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2+cu118) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2+cu118) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.2+cu118) (1.3.0)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [torchaudio]\n",
            "\u001b[1A\u001b[2KSuccessfully installed torch-2.1.2+cu118 torchaudio-2.1.2+cu118 torchvision-0.16.2+cu118\n",
            "Collecting transformers==4.38.2\n",
            "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
            "Collecting accelerate==0.25.0\n",
            "  Using cached accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting huggingface-hub==0.27.1\n",
            "  Using cached huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n",
            "  Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.27.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.27.1) (4.13.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.25.0) (2.1.2+cu118)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (3.1.6)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.25.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.25.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate==0.25.0) (1.3.0)\n",
            "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
            "Using cached huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
            "Using cached accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Installing collected packages: huggingface-hub, tokenizers, accelerate, transformers\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [transformers]\n",
            "\u001b[1A\u001b[2KSuccessfully installed accelerate-0.25.0 huggingface-hub-0.27.1 tokenizers-0.15.2 transformers-4.38.2\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: rouge-metric in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI7U51uL7StI",
        "outputId": "b337901b-9479-41fe-fdc4-4059c5ce94d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UW-LING-573-Group-5'...\n",
            "remote: Enumerating objects: 519, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 519 (delta 33), reused 32 (delta 17), pack-reused 457 (from 1)\u001b[K\n",
            "Receiving objects: 100% (519/519), 391.17 KiB | 16.30 MiB/s, done.\n",
            "Resolving deltas: 100% (99/99), done.\n",
            "mv: cannot move 'UW-LING-573-Group-5' to 'UW_LING_573_Group_5/UW-LING-573-Group-5': Directory not empty\n"
          ]
        }
      ],
      "source": [
        "# Clone repo & set up environment\n",
        "\n",
        "!rm -rf UW-LING-573-Group-5\n",
        "!git clone https://github.com/chelsk5/UW-LING-573-Group-5.git\n",
        "!mv UW-LING-573-Group-5 UW_LING_573_Group_5 # Python modules use underscores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "02E-ajOr9_7_",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9db10524-3e10-4d44-bc6b-c23df24ab57a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UW_LING_573_Group_5\n",
            "LING573_Model.ipynb  requirements.txt  summaries.json\n",
            "Minutes.md           rouge_eval.py     tf-idf.py\n",
            "README.md            rouge.py          \u001b[0m\u001b[01;34mUW-LING-573-Group-5\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%cd UW_LING_573_Group_5/\n",
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKDzu-BIKI_v"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "53N0paoCC2gb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce02398-4de3-4aa8-bba8-cf8bad180045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 1. Load data\n",
        "from datasets import load_dataset\n",
        "opinosis = load_dataset(\"kavgan/opinosis\", split=\"train\") # HuggingFace hub\n",
        "dataset = opinosis.train_test_split(test_size=0.2,seed=1) # follow common split ratio\n",
        "train_data = dataset[\"train\"]\n",
        "test_data = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Preprocessing\n",
        "def clean_data(sent_to_sum):\n",
        "  \"\"\"Cleans the given review and returns a dictionary\n",
        "  of aggregated reviews and the first gold summary\"\"\"\n",
        "  reviews = sent_to_sum[\"review_sents\"].strip()\n",
        "  # Choose the first gold summary\n",
        "  gold_summary = sent_to_sum[\"summaries\"][0].strip()\n",
        "  return {\n",
        "      \"reviews\": reviews,\n",
        "      \"gold_summary\": gold_summary\n",
        "  }\n",
        "\n",
        "# Sanity check!\n",
        "# Apply cleaning to a small sample\n",
        "cleaned_train = train_data.map(clean_data)\n",
        "cleaned_test = test_data.map(clean_data)\n",
        "\n",
        "# Print the result\n",
        "print(\"Input reviews:\")\n",
        "print(cleaned_train[0][\"reviews\"])\n",
        "print(\"\\nTarget summary:\")\n",
        "print(cleaned_train[0][\"gold_summary\"])\n",
        "\n",
        "print(\"\\nInput reviews:\")\n",
        "print(cleaned_train[1][\"reviews\"])\n",
        "print(\"\\nTarget summary:\")\n",
        "print(cleaned_train[1][\"gold_summary\"])"
      ],
      "metadata": {
        "id": "uburJXDhoURE",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561117aa-64ed-4b3e-f6ac-c7c75daa2625"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input reviews:\n",
            "The food for our event was delicious .\r\n",
            " The food in the lounge was great and very fresh, , , salads, sandwiches etc .\r\n",
            " As far as food, walk a few blocks toward Michigan Ave turn left or right and there are plently of less expensive places to eat .\r\n",
            " The Palm resturant in the hotel had some specials Sunday night, we ate there and the food service,etc were outstanding portions are large and we shared since we are not big eaters .\r\n",
            " Took the charge of the minibar which we had used to keep my 2 year old sons food .\r\n",
            " We never ate anything onsite so I can't vouch for the food options immediately avialable .\r\n",
            " The Lobby bar does not serve food very late at night and we couldn't find any vending machines for soda or snacks, so stop at a nearby market before going in for the night .\r\n",
            " In any case, I had allotted approximately $150 for food for 4 days 3 nights .\r\n",
            " But the presentation was awesome and the food was so good and not having to walk in the dark at night and scare my mother or pay for a taxi or bus, but sit around and eat comfortably in my pajamas was well worth the cost to me .\r\n",
            " The food was delicious and the view was fabulous .\r\n",
            " For self catering types, Bockwinkels is a well stocked mini grocery 2 blocks south and 1 block east on street level for snacks, drinks and all kinds of deli prepared foods, plus the Aon Center's underground food court 2 blocks south and 2 .\r\n",
            " Room service was extortionate though, very very expensive, so we didnt bother, as food outlets a few minutes walk away .\r\n",
            " We also did the chicago food tasting tour which you can find here on travelzoo, this was fantastic and well worth the $40 pp .\r\n",
            " So if you move around bottles in the fridge so that you can store your own food personal items, the sensor will send a signal to the hotel and you will be charged .\r\n",
            " We passed on the restaurants simply because the cost of breakfast was insane and the food tends to be mediocre .\r\n",
            " Also don't forget to go the &quot The River North&quot  for great food .\r\n",
            " An added bonus was the Executive Lounge with complimentary food throughout the day .\r\n",
            " Room service food quality was very good but with a $7 tray charge plus 20% service, I then absolutely refuse to tip for delivery .\r\n",
            " there is a market one block south of the Hyatt that has wine beer and food for reasonable prices until 2am, and a Dominick's   grocery store across the river open til 1am .\r\n",
            " Had an appetizer at the Lobby Lounge, and the food came from the Palm Restaurant .\r\n",
            " The hotel buffet had fabulous food .\r\n",
            " I didn't dine in the hotel, so I can't tell you about the food .\r\n",
            " We enjoyed the breakfast although the other evening food service was a bit skimpy .\r\n",
            " Room service got my breakfast order wrong but they made it up the next day when I ordered a kids meal and got enough food for three people .\r\n",
            " I was delighted as the waiters were very friendly and the seafood was delicious .\r\n",
            " mile with traditional Italian food and a great atmosphere .\r\n",
            " A nice restaurant with reasonable prices and great food .\r\n",
            " Good but pricey food, great breakfast .\r\n",
            " My only complaint is the Palm restaurant, it is expensive and the food not very good .\r\n",
            " If you want good food at a decent price, there is a restaurant called Houlihan's down the street from the hotel on Wacker St .\r\n",
            " We ordered room service for late night ice cream and fruit and also breakfast and it was prompt and very excellent food .\r\n",
            "Ed Debevics is fun, , sassy servers, diner food, total check about $30 for all four of us .\r\n",
            "Brain Food Cafe in the museum of Science and Industry, , sounds crazy I know but it was the best museum food ever .\r\n",
            " No line, a plate full of REAL food with drink included for $7 each .\r\n",
            "Staying with food, try also the Weber Grill, N .\r\n",
            "The few complaints I had were as follows ,  refrigerator was stocked with food that was weight sensitive .\r\n",
            " But the food was good and fresh .\r\n",
            " Very friendly servers and nice selection of food at a reasonable price .\r\n",
            " We called several times and within 30 minutes our food was at the door .\r\n",
            " The Italian food was superb .\r\n",
            " My wife and I are food snobs, and this place really was a treat .\r\n",
            " Very much enjoyed the Chicago Food Tour, a walking tour that incorporates food tastings at various places like a bakery, traditional deli, pizza, chocolate lounge, etc .\r\n",
            " Room service was efficient, average priced, and decent food .\r\n",
            " I ate their last night and the food and service were top noch .\r\n",
            "I had a great experience here from the quality of the lobby, to the quality of the room, to the view, to the room service, to the food .\r\n",
            " The hotel is advertised to have multiple restaurants, but that was not the case either , ,  other than room service the only option for dinner is The Palms restaurant which is very overpriced for the quality of the food .\r\n",
            " There's an escalator that goes into the basement where you'll find a nice upscale restaurant and a long walkway concourse that leads to an underground mini, mall, with a grocery store, fast, food restaurants, barber shop etc .\r\n",
            ", followed shortly thereafter by a knock and a housekeeper telling us our sign wasn't out  the hotel's extremely limited cable service    our housekeeper didn't refill replace our shampoo or shower gel our entire stay, nor did she empty all of our trash bins at any one time  our room service on two occasions brought us cold soggy, , and very late, , food  the lobby bar closes at 9 p .\r\n",
            " Food unexceptional although the breakfast buffet in the basement restaurant   was of good quality .\r\n",
            " We were able to get a crib, a play pen, and a small room refrigerator for storing baby food at NO additional cost .\r\n",
            " The food at the Swissotel is better than the average hotel but nothing special .\n",
            "\n",
            "Target summary:\n",
            "The presentation was awesome and the food is good.\r\n",
            "Room service has good food quality, but it's fairly expensive.\n",
            "\n",
            "Input reviews:\n",
            "Parking was expensive but I think this is common for San Fran .\r\n",
            " there is a fee for parking but well worth it seeing no where to park if you do have a car .\r\n",
            " The parking was free, which was great, and the hotel was conveniently located for public transport, and local attractions .\r\n",
            "As for in, and, out parking, I have seen a lot of San Francisco with no car at all .\r\n",
            " They have a parking garage, but they make you leave your vehicle for them to park   and then if you want to take a drive later, you have to wait for the staff to get it .\r\n",
            " There is no real parking space, so I had to pull up in front of the hotel in a small space .\r\n",
            "There was valet parking at a cost of $42 .\r\n",
            " Rooms are very comfortable and we were charged $15 for parking our motorcycle in the garage .\r\n",
            " Our ONLY complaint was one doorman valet wasn't thrilled about us parking the car at the front \r\n",
            " Parking is easier then you might realize .\r\n",
            " There is a parking ramp across the street that is MUCH cheaper then hotel parking .\r\n",
            " Our room rate was all in order  wed booked a TravelZoo special, and the front desk had the right rate, and the right parking discount .\r\n",
            " Of course you have the added thrill of paying over $40 a night for parking and whatever your chiropractor bill is when you are done with the beds .\r\n",
            "I found a great mid week deal that included a discount on parking .\r\n",
            "On the other hand, the valet parking attendant bell people were very friendly .\r\n",
            "Parking is expensive, Rooms are on the small side .\r\n",
            " However, parking is $30 night and valet only, which becomes a pain .\r\n",
            "Only other downer was $38 per night for valet parking .\r\n",
            "room was quiet, clean with good beds but very very smallnot a cheap hotelvalit parking was to much expencive but oke its the location\r\n",
            " Parking is crazy expensive, we drove up from LA so we had a car ,  but otherwise, you really don't need it .\r\n",
            " Parking is expensive   and everything is very much within walking distance .\r\n",
            " Valet parking is, in my view, unreasonably high at $38 per night, but again we knew this before we arrived so can't really complain .\r\n",
            " As most hotels in the city, parking is expensive so unless you really need a vehicle its cheaper to just buy a MUNI Passport during your stay .\r\n",
            " The hotel does have parking, but it is expensive and hard to get around the city while driving .\r\n",
            " The parking was steep  , so if I had to do it again I would have parked 1 2 block north for about half of that .\r\n",
            " Parking at $45 a night was outrageous, but probably worth it to keep your car safe .\r\n",
            " We did not have a car, hotel parking is expensive .\r\n",
            " The valet parking was very convenient and seemed secure as it is attached to the hotel .\r\n",
            " A lot of hotel parking garages lots are not on the same premises as the hotel in the area .\r\n",
            " First, the biggest &quot gotcha&quot  is the mandatory valet parking .\r\n",
            " Not trying to bad mouth the Tuscan, it is a nice place to stay, and if you're not put off by the valet parking, you shouldn't be too disappointed .\r\n",
            " Valet parking for $40 per night seems a bit steep,but at least the service was efficient and quick .\r\n",
            " The prize for vallet parking is steep but I think you can expect that anywhere you go in San Francisco .\r\n",
            " Valet parking was $45 per night with tax, but that includes in and out privileges and my car was there whenever I needed it .\r\n",
            " Watch out for steep parking rates of about $45 per night .\r\n",
            " This total includes the taxes and tips for mandatory valet parking .\r\n",
            "Aside from the parking rate, you won't be disappointed .\r\n",
            " The other thing that I found disappointing was I can't remember the exact details but something like the charge they quote for parking   was even more than that because there was some sort of a tax .\r\n",
            " Onsite parking is available .\r\n",
            " I called before we ever arrived to inquire about it and was told that parking was no problem at all .\r\n",
            " I guess calling to inquire about parking while giving them our name and reservation dates didn't warrant the clerk to inform us that we should make a reservation for our car also .\r\n",
            " Valet parking is provided at $36 day which was, I thought, excessive .\r\n",
            "Walgreens has same souvenirs the airport shops do for 1 2 $Do not rent a car, waste of time, money,   no parking !\r\n",
            "The valet parking was overpriced, the service was terrible, and the coffe was cold .\r\n",
            " At the hotel, it is valet only parking .\r\n",
            " Check for Internet deals and don't pay for their parking .\r\n",
            " I do think that if you are staying at a hotel you should not have to pay for parking .\r\n",
            "The hotel, located a few blocks from Fisherman's Wharf as well as Ghiardelli Square, has a nice Italian theme that carries into the lobby as well as the uniforms for the parking valets .\r\n",
            " Speaking of parking, you're going to have to pay and it's going to have to be valet service, as there are no free self park lots that are part of the hotel .\r\n",
            " Not only is it pricey at $36 day, but having nothing but valet parking means you have no access to your car unless you have one of the valets fetch it from the lot for you .\r\n",
            " Although our room was indeed non, smoking   and we had a king size bed we ended up on the 3rd floor and our room, facing Mason Street, overlooked a concrete multi, story parking lot although if you stood at the window you could see the Coit Tower which was a nicer view !\r\n",
            " Several have mentioned the charge for parking .\r\n",
            " It really does save if you can get a deal with free parking, but the fee is not out of line .\r\n",
            " Valet parking was $36 a day, but we're not complaining because other SF wharf hotels have charged us a similar fee even without the valet service .\r\n",
            " Another irritant was the crazy parking fees for the hotel .\r\n",
            " They do a good job and are a heck of a lot cheaper than a cab and you wont have to mess with the parking fees .\r\n",
            " When I got a GREAT deal $99 INCLUDING parking !\r\n",
            "The great thing for us was the free parking .\r\n",
            " Normally parking is $36 a night so getting that thrown in was a fantastic steal .\r\n",
            "We stayed at the BW Tuscan Inn only because they were offering a great winter deal , $99 for a room with king, size bed and free parking   .\r\n",
            " Overall, we would recommend this place to friends, especially if they are able to get a good discount and free parking .\r\n",
            " Nice rooms, a bit on the smaller side, but parking right at the hotel .\r\n",
            " staff were helpful although the valet parking and the laundry costs were extremely expensive .\r\n",
            " They allowed this, so we only had to pay the regular parking fee of the hotel .\r\n",
            " Way too much money to pay for parking, but that is the way it is in San Francisco .\r\n",
            " You don't need a car because parking is a nightmare .\r\n",
            "all these hotels charge parking fees .\r\n",
            " Parking at the hotel is expensive at $32 plus tax per night but the up side is that you have in out privilages .\r\n",
            "Parking in the city is a royal pain !\r\n",
            " Although the parking charge is steep, it seems to be the market price in San Francisco, or as the locals like to call it :\r\n",
            " I notice on other reviews the high cost of parking which we didn't have to consider as we were advised you do not need a car to get around SF, and this is quite true .\r\n",
            " All in all, the location was perfect ,  a short walk to the cable cars or bus lines, a Safeway grocery and Walgreens drugs is across the street in a parking garage, type mall, and there are small places to eat breakfast nearby plus an IHOP a few blocks over .\r\n",
            " Parking at the hotel was a hefty $32 per day with unlimited in and out privileges .\r\n",
            " Parking is an issue in SF so either pay the $30 .\r\n",
            " In addition, the valet parking is apparently handled by an outside contractor, and turned out to be considerably more expensive than we had been told ,  $35 day ,  which seemed very high .\r\n",
            " Because of the exhorbitant $29 valet parking fee, we only rented a car for Sonoma and Napa for one day .\r\n",
            " You'll be having fun while some other poor sod is looking for a parking space that isn't there .\r\n",
            "30 am, we discovered that the parking garage next door   would not be open until 7 ,  7 :\r\n",
            " I thought the $29 per day parking was ridiculous, but I hear that's the standard in SF .\r\n",
            "Upon arriving, there was a very long and confusing to do with the valet parking ,  we hadn't gotten our luggage out of th car and the valet wanted the keys .\r\n",
            " There was a fee of $29 per day for the parking .\r\n",
            " Valet parking attendant was no better .\r\n",
            " paid $161 plus tax along with a $20 parking fee .\r\n",
            "As far as parking is concerned, we were shell, shocked at what most of the hotels charge for parkingup to $40 night .\r\n",
            " The Tuscan is $29 night, but if you search for a package rate on their web site, you may get parking included in your room rate .\r\n",
            " And, conveniently, there is a Safeway and a drug store hidden in the parking garage across the street .\r\n",
            " We didnt have or need a car so we didnt have to deal with parking a car I believe it is $30 a day, ,  par for the city .\r\n",
            " It is located 2 blocks from the wharf which was outstanding considering parking is hard to come by .\r\n",
            "Note to those with a car, overnight valet parking is $26 but right next door the car park is $15 and it takes about 30 seconds to get from your car back to the hotel ,  why pay $26 ?\r\n",
            " Parking was $26 day ,  free valet with on and off priv .\r\n",
            " Make sure you realize that you pay for parking .\r\n",
            "00 for valet parking, but that was okay .\r\n",
            " I was aupset, since my $89 night room had gone to $138 night between the parking   and the pet charge .\r\n",
            " We didn't pull our car out of the parking garage the whole time we were there .\r\n",
            " Parking is not cheap, check before you go .\r\n",
            " Parking was only $10 with AAA rate .\r\n",
            " Hotel parking   is now $23 a night .\n",
            "\n",
            "Target summary:\n",
            "Parking is expensive.\r\n",
            "Best to get free parking.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load tokenizer & model\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
        "\n",
        "# quantization_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_compute_dtype=\"float16\",\n",
        "#     bnb_4bit_quant_type=\"nf4\",  # You can also use \"fp4\" depending on your needs\n",
        "#     bnb_4bit_use_double_quant=True\n",
        "# )\n",
        "\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"auto\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "Hzps4oQKgYG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c3dcb29-7340-4b73-b7e3-499a45563bb0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Tokenize reviews & gold summaries\n",
        "def tokenize_data(cleaned_data):\n",
        "  \"\"\"Convert the raw text into token IDs for model processing\"\"\"\n",
        "  # Prepend task for T5 to execute\n",
        "  pref_text = \"summarize: \" + cleaned_data[\"reviews\"]\n",
        "\n",
        "  # Tokenize input (reviews)\n",
        "  # Ensure that text does not exceed (is in fact exactly) maximum of 512 tokens\n",
        "  tokenized_input = tokenizer(pref_text, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "  # Tokenize target output (gold summary)\n",
        "  gold_summary = cleaned_data[\"gold_summary\"]\n",
        "  labels = tokenizer([gold_summary], max_length=64, truncation=True) # expects a list\n",
        "\n",
        "  # Add target labels to the model input\n",
        "  # Format for HuggingFace Trainer\n",
        "  tokenized_input[\"labels\"] = labels[\"input_ids\"][0] # get first (& only) item of nested list\n",
        "  return tokenized_input\n",
        "\n",
        "#apply tokenization to data\n",
        "tokenized_train = cleaned_train.map(tokenize_data, batched=False) # one row at a time\n",
        "tokenized_test = cleaned_test.map(tokenize_data, batched=False)\n",
        "\n",
        "#check tokenization\n",
        "print(\"Tokenized input example (input_ids):\")\n",
        "print(tokenized_train[0][\"input_ids\"])\n",
        "print(\"Tokenized target (labels):\")\n",
        "print(tokenized_train[0][\"labels\"])"
      ],
      "metadata": {
        "id": "bzweEIDlGItS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "526eb3a5f8994ca4973f3e57d3ed34c7",
            "65031d0c97944e04b662668e1ac570d5",
            "5af85ee86131408bb01b5908b2031b82",
            "c75edbdac896400291f7b82f44feffc0",
            "7e26880af48d49f9984bdfe429f878e4",
            "0830340e22a14b69b13d8b70853cef5f",
            "76fbfddf9af344baa30ad1e47c5f5617",
            "660221be809645aca8d17b0ad74571df",
            "394cdf5e77814174a958e1368fab372b",
            "33431d2c30e94b0bae3de7e1a4446fe1",
            "9b2e7d739a6d439f9087643e48ad4003"
          ]
        },
        "outputId": "3391459e-09d7-441f-8c95-fdfd9a3f1f79"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "526eb3a5f8994ca4973f3e57d3ed34c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized input example (input_ids):\n",
            "[21603, 10, 37, 542, 21, 69, 605, 47, 3326, 3, 5, 37, 542, 16, 8, 9658, 47, 248, 11, 182, 1434, 6, 3, 6, 3, 6, 5870, 7, 6, 20017, 672, 3, 5, 282, 623, 38, 542, 6, 1482, 3, 9, 360, 6438, 2957, 5847, 8945, 919, 646, 42, 269, 11, 132, 33, 4752, 295, 120, 13, 705, 2881, 1747, 12, 3, 1544, 3, 5, 37, 8005, 880, 450, 288, 16, 8, 1595, 141, 128, 534, 7, 1771, 706, 6, 62, 3, 342, 132, 11, 8, 542, 313, 6, 21117, 130, 4590, 17622, 33, 508, 11, 62, 2471, 437, 62, 33, 59, 600, 23852, 7, 3, 5, 304, 1825, 8, 1567, 13, 8, 3016, 1047, 84, 62, 141, 261, 12, 453, 82, 204, 215, 625, 520, 7, 542, 3, 5, 101, 470, 3, 342, 959, 3, 24452, 78, 27, 54, 31, 17, 3, 1621, 2295, 21, 8, 542, 931, 2017, 3, 2960, 138, 179, 3, 5, 37, 17464, 969, 1207, 405, 59, 1716, 542, 182, 1480, 44, 706, 11, 62, 2654, 31, 17, 253, 136, 19518, 53, 4096, 21, 12495, 42, 12751, 6, 78, 1190, 44, 3, 9, 4676, 512, 274, 352, 16, 21, 8, 706, 3, 5, 86, 136, 495, 6, 27, 141, 66, 14708, 26, 3241, 25689, 21, 3, 2, 12437, 2, 21, 314, 477, 220, 8348, 3, 5, 299, 8, 3831, 47, 2879, 11, 8, 542, 47, 78, 207, 11, 59, 578, 12, 1482, 16, 8, 2164, 44, 706, 11, 6541, 15, 82, 2039, 42, 726, 21, 3, 9, 9256, 42, 2601, 6, 68, 2561, 300, 11, 3, 1544, 17540, 16, 82, 2576, 1191, 2754, 47, 168, 1494, 8, 583, 12, 140, 3, 5, 37, 542, 47, 3326, 11, 8, 903, 47, 7284, 3, 5, 242, 1044, 14825, 1308, 6, 272, 3961, 29046, 7, 19, 3, 9, 168, 3, 23378, 3016, 11426, 204, 6438, 3414, 11, 209, 2463, 5727, 30, 2815, 593, 21, 12751, 6, 6750, 11, 66, 4217, 13, 3, 12460, 2657, 4371, 6, 303, 8, 71, 106, 1166, 31, 7, 13104, 542, 1614, 204, 6438, 3414, 11, 204, 3, 5, 4181, 313, 47, 3, 10398, 127, 1575, 342, 713, 6, 182, 182, 2881, 6, 78, 62, 737, 17, 13965, 6, 38, 542, 14290, 3, 9, 360, 676, 1482, 550, 3, 5, 101, 92, 410, 8, 8780, 9, 839, 542, 12246, 1552, 84, 25, 54, 253, 270, 30, 1111, 172, 32, 32, 6, 48, 47, 2723, 11, 168, 1494, 8, 23853, 3, 1572, 3, 5, 264, 3, 99, 25, 888, 300, 10787, 16, 8, 11416, 78, 24, 25, 54, 1078, 39, 293, 542, 525, 1173, 6, 8, 7824, 56, 1299, 3, 9, 3240, 12, 8, 1595, 11, 25, 56, 36, 4977, 3, 5, 101, 2804, 30, 8, 3661, 914, 250, 8, 583, 13, 3688, 47, 21070, 11, 8, 542, 2134, 7, 12, 36, 3, 28595, 15, 3, 5, 1203, 278, 31, 17, 2612, 12, 281, 8, 3, 184, 8270, 37, 2473, 1117, 184, 8270, 21, 248, 542, 3, 5, 389, 974, 4023, 47, 8, 5408, 18854, 28, 13968, 542, 1019, 8, 239, 3, 5, 4181, 313, 542, 1]\n",
            "Tokenized target (labels):\n",
            "[37, 3831, 47, 2879, 11, 8, 542, 19, 207, 5, 4181, 313, 65, 207, 542, 463, 6, 68, 34, 31, 7, 5163, 2881, 5, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Create batch of examples\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "# Dynamically pad sentences to the longest length\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "NkSfR_pxLUWi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Evaluate model using ROUGE\n",
        "!pip install rouge-score evaluate\n",
        "import numpy as np\n",
        "from UW_LING_573_Group_5 import rouge\n",
        "from rouge import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "\n",
        "def compute_rouge(eval_pred):\n",
        "  \"\"\"Decodes token IDs and computes the ROUGE scores of generated summaries\"\"\"\n",
        "  predictions, labels = eval_pred # model outputs & gold summaries as token IDs\n",
        "  # Decode model output IDs back into text\n",
        "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "  # Replace padding tokens IDs (-100) with tokenizer's pad token ID (usually 0)\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  # Decode gold summary IDs back into text\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  # Compute precision, recall, & f1 across ROUGE scores\n",
        "  rouge1_p, rouge1_r, rouge1_f = [], [], []\n",
        "  rouge2_p, rouge2_r, rouge2_f = [], [], []\n",
        "  rougeL_p, rougeL_r, rougeL_f = [], [], []\n",
        "\n",
        "  for pred, label in zip(decoded_preds, decoded_labels):\n",
        "    scores = scorer.score(pred, label)\n",
        "    # ROUGE-1\n",
        "    rouge1_p.append(scores[\"rouge1\"].precision)\n",
        "    rouge1_r.append(scores[\"rouge1\"].recall)\n",
        "    rouge1_f.append(scores[\"rouge1\"].fmeasure)\n",
        "\n",
        "    # ROUGE-2\n",
        "    rouge2_p.append(scores[\"rouge2\"].precision)\n",
        "    rouge2_r.append(scores[\"rouge2\"].recall)\n",
        "    rouge2_f.append(scores[\"rouge2\"].fmeasure)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rougeL_p.append(scores[\"rougeL\"].precision)\n",
        "    rougeL_r.append(scores[\"rougeL\"].recall)\n",
        "    rougeL_f.append(scores[\"rougeL\"].fmeasure)\n",
        "\n",
        "  print(\"\\n--- ORIGINAL ---\")\n",
        "  print(\"Prediction:\", decoded_preds[0])\n",
        "  print(\"Gold:\", decoded_labels[0])\n",
        "\n",
        "  print(\"\\n--- HF-style Preprocessed ---\")\n",
        "  print(\"Pred:\", preprocess(decoded_preds[0]))\n",
        "  print(\"Gold:\", preprocess(decoded_labels[0]))\n",
        "\n",
        "  # Return rounded mean across scores\n",
        "  return {\n",
        "      \"rouge1_precision\": round(np.mean(rouge1_p), 4),\n",
        "      \"rouge1_recall\": round(np.mean(rouge1_r), 4),\n",
        "      \"rouge1_f1\": round(np.mean(rouge1_f), 4),\n",
        "      \"rouge2_precision\": round(np.mean(rouge2_p), 4),\n",
        "      \"rouge2_recall\": round(np.mean(rouge2_r), 4),\n",
        "      \"rouge2_f1\": round(np.mean(rouge2_f), 4),\n",
        "      \"rougeL_precision\": round(np.mean(rougeL_p), 4),\n",
        "      \"rougeL_recall\": round(np.mean(rougeL_r), 4),\n",
        "      \"rougeL_f1\": round(np.mean(rougeL_f), 4),\n",
        "  }\n",
        "\n",
        "# The code below is only useable if this ROUGE runtime error gets resolved:\n",
        "# https://huggingface.co/evaluate-metric?sort_spaces=modified#spaces\n",
        "\n",
        "# import evaluate\n",
        "\n",
        "# rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# def compute_rouge(eval_pred):\n",
        "#   \"\"\"Decodes token IDs and computes the ROUGE score of generated summaries\"\"\"\n",
        "#   predictions, labels = eval_pred # model outputs & gold summaries as token IDs\n",
        "#   # Decode model output IDs back into text\n",
        "#   decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "#   # Replace padding tokens IDs (-100) with tokenizer's pad token ID (usually 0)\n",
        "#   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "#   # Decode gold summary IDs back into text\n",
        "#   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "#   # Compare model output against gold labels via ROUGE\n",
        "#   result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "#   clean_result = {k: round(v, 4) for k,v in result.items()} # map rouge type to value\n",
        "#   return clean_result"
      ],
      "metadata": {
        "id": "H5gWlwSRRsSR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d86a89b-611e-4989-e173-9f58294a4aa9",
        "collapsed": true
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets>=2.0.0->evaluate) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6B. Alternative ROUGE evaluation using rouge_metric PyRouge\n",
        "import numpy as np\n",
        "from rouge_metric import PyRouge\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "rouge = PyRouge(\n",
        "    rouge_n=(1, 2, 4),\n",
        "    rouge_l=True,\n",
        "    rouge_w=True,\n",
        "    rouge_s=True,\n",
        "    rouge_su=True,\n",
        "    skip_gap=4\n",
        ")\n",
        "\n",
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  tokens = text.split()\n",
        "  stemmed = [stemmer.stem(t) for t in tokens]\n",
        "  return ' '.join(stemmed)\n",
        "  # return text.strip() # apply minimal preprocessing to match HF\n",
        "\n",
        "def compute_rouge(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  hypotheses = [preprocess(pred) for pred in decoded_preds]\n",
        "  references = [[preprocess(label)] for label in decoded_labels]\n",
        "\n",
        "  #scores = rouge.get_scores(hypotheses, references, avg=True)\n",
        "  scores = rouge.evaluate_tokenized(hypotheses, references)\n",
        "\n",
        "  print(\"\\n--- ORIGINAL ---\")\n",
        "  print(\"Prediction:\", decoded_preds[0])\n",
        "  print(\"Gold:\", decoded_labels[0])\n",
        "\n",
        "  print(\"\\n--- PyROUGE-style Preprocessed ---\")\n",
        "  print(\"Pred:\", preprocess(decoded_preds[0]))\n",
        "  print(\"Gold:\", preprocess(decoded_labels[0]))\n",
        "\n",
        "  return {\n",
        "      \"rouge1_precision\": round(scores[\"rouge-1\"][\"p\"], 4),\n",
        "      \"rouge1_recall\": round(scores[\"rouge-1\"][\"r\"], 4),\n",
        "      \"rouge1_f1\": round(scores[\"rouge-1\"][\"f\"], 4),\n",
        "      \"rouge2_precision\": round(scores[\"rouge-2\"][\"p\"], 4),\n",
        "      \"rouge2_recall\": round(scores[\"rouge-2\"][\"r\"], 4),\n",
        "      \"rouge2_f1\": round(scores[\"rouge-2\"][\"f\"], 4),\n",
        "      \"rougeL_precision\": round(scores[\"rouge-l\"][\"p\"], 4),\n",
        "      \"rougeL_recall\": round(scores[\"rouge-l\"][\"r\"], 4),\n",
        "      \"rougeL_f1\": round(scores[\"rouge-l\"][\"f\"], 4),\n",
        "      \"rougeSU4_precision\": round(scores[\"rouge-su4\"][\"p\"], 4),\n",
        "      \"rougeSU4_recall\": round(scores[\"rouge-su4\"][\"r\"], 4),\n",
        "      \"rougeSU4_f1\": round(scores[\"rouge-su4\"][\"f\"], 4),\n",
        "  }"
      ],
      "metadata": {
        "id": "cfBUcPpBOEV2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train model\n",
        "# !pip install --upgrade accelerate==0.27.2\n",
        "\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "# Manually tune hyperparameters\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./t5-small-results\", # folder to save progress\n",
        "    evaluation_strategy=\"epoch\", # evaluate after one complete pass\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8, # small batch size for small data\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3, # number of checkpoints (subject to change?)\n",
        "    num_train_epochs=3, # total passes thru data (also stc...)\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,# changed to False to avoid ValueError\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_rouge\n",
        ")\n",
        "\n",
        "# Train (finally)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "lsiz5KlGtfqz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "outputId": "80242436-0e03-4868-8674-8fc15d4f4c7d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 00:11, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1 Precision</th>\n",
              "      <th>Rouge1 Recall</th>\n",
              "      <th>Rouge1 F1</th>\n",
              "      <th>Rouge2 Precision</th>\n",
              "      <th>Rouge2 Recall</th>\n",
              "      <th>Rouge2 F1</th>\n",
              "      <th>Rougel Precision</th>\n",
              "      <th>Rougel Recall</th>\n",
              "      <th>Rougel F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.350780</td>\n",
              "      <td>0.232100</td>\n",
              "      <td>0.244500</td>\n",
              "      <td>0.222800</td>\n",
              "      <td>0.073400</td>\n",
              "      <td>0.060500</td>\n",
              "      <td>0.062300</td>\n",
              "      <td>0.208700</td>\n",
              "      <td>0.204800</td>\n",
              "      <td>0.194400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.335762</td>\n",
              "      <td>0.232100</td>\n",
              "      <td>0.244500</td>\n",
              "      <td>0.222800</td>\n",
              "      <td>0.073400</td>\n",
              "      <td>0.060500</td>\n",
              "      <td>0.062300</td>\n",
              "      <td>0.208700</td>\n",
              "      <td>0.204800</td>\n",
              "      <td>0.194400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.329674</td>\n",
              "      <td>0.232100</td>\n",
              "      <td>0.241400</td>\n",
              "      <td>0.221300</td>\n",
              "      <td>0.073400</td>\n",
              "      <td>0.059700</td>\n",
              "      <td>0.062000</td>\n",
              "      <td>0.208700</td>\n",
              "      <td>0.202000</td>\n",
              "      <td>0.193000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ORIGINAL ---\n",
            "Prediction: If a case was included, as with the Kindle 1, that would have been reflected\n",
            "Gold: While the price of the Amazon Kindle 2 is high, it is worth it. That said, beware: the prices of recently released books are high.\n",
            "\n",
            "--- HF-style Preprocessed ---\n",
            "Pred: if a case wa includ as with the kindl 1 that would have been reflect\n",
            "Gold: while the price of the amazon kindl 2 is high it is worth it that said bewar the price of recent releas book are high\n",
            "\n",
            "--- ORIGINAL ---\n",
            "Prediction: If a case was included, as with the Kindle 1, that would have been reflected\n",
            "Gold: While the price of the Amazon Kindle 2 is high, it is worth it. That said, beware: the prices of recently released books are high.\n",
            "\n",
            "--- HF-style Preprocessed ---\n",
            "Pred: if a case wa includ as with the kindl 1 that would have been reflect\n",
            "Gold: while the price of the amazon kindl 2 is high it is worth it that said bewar the price of recent releas book are high\n",
            "\n",
            "--- ORIGINAL ---\n",
            "Prediction: If a case was included, as with the Kindle 1, that would have been reflected\n",
            "Gold: While the price of the Amazon Kindle 2 is high, it is worth it. That said, beware: the prices of recently released books are high.\n",
            "\n",
            "--- HF-style Preprocessed ---\n",
            "Pred: if a case wa includ as with the kindl 1 that would have been reflect\n",
            "Gold: while the price of the amazon kindl 2 is high it is worth it that said bewar the price of recent releas book are high\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=15, training_loss=2.952422332763672, metrics={'train_runtime': 11.7984, 'train_samples_per_second': 10.171, 'train_steps_per_second': 1.271, 'total_flos': 16241016176640.0, 'train_loss': 2.952422332763672, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Log metrics to JSON file\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics) # sanity check\n",
        "\n",
        "import json\n",
        "with open(\"t5-small-results/metrics.json\", \"w\") as f:\n",
        "  json.dump(metrics, f, indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "7HAIpeaIMrff",
        "outputId": "610e2c80-c389-4675-9ed8-11964e22cf23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 3.914048910140991, 'eval_rouge1_precision': 0.7281, 'eval_rouge1_recall': 0.71, 'eval_rouge1_f1': 0.7189, 'eval_rouge2_precision': 0.4209, 'eval_rouge2_recall': 0.386, 'eval_rouge2_f1': 0.4027, 'eval_rougeL_precision': 0.7281, 'eval_rougeL_recall': 0.71, 'eval_rougeL_f1': 0.7189, 'eval_rougeSU4_precision': 0.5528, 'eval_rougeSU4_recall': 0.5251, 'eval_rougeSU4_f1': 0.5386, 'eval_runtime': 15.7814, 'eval_samples_per_second': 0.697, 'eval_steps_per_second': 0.127, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Save predictions & labels in separate files\n",
        "import os\n",
        "system_dir = \"t5-small-results/system_summaries\" # t5 predictions\n",
        "model_dir = \"t5-small-results/model_summaries\" # gold reference\n",
        "os.makedirs(system_dir, exist_ok=True)\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Maybe this can be streamlined later since it's handled by compute_rouge()...\n",
        "predictions = trainer.predict(tokenized_test)\n",
        "decoded_preds = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)\n",
        "labels = np.where(predictions.label_ids != -100, predictions.label_ids, tokenizer.pad_token_id)\n",
        "decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "for i, (pred, ref) in enumerate(zip(decoded_preds, decoded_labels)):\n",
        "    with open(f\"{system_dir}/{i}.txt\", \"w\") as sys_f:\n",
        "        sys_f.write(pred.strip() + \"\\n\")\n",
        "    with open(f\"{model_dir}/{i}.A.1.txt\", \"w\") as ref_f:\n",
        "        ref_f.write(ref.strip() + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3pRwxcFBbeHL",
        "outputId": "ebd43d52-40d8-47a9-ff2c-8db98588a083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "526eb3a5f8994ca4973f3e57d3ed34c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65031d0c97944e04b662668e1ac570d5",
              "IPY_MODEL_5af85ee86131408bb01b5908b2031b82",
              "IPY_MODEL_c75edbdac896400291f7b82f44feffc0"
            ],
            "layout": "IPY_MODEL_7e26880af48d49f9984bdfe429f878e4"
          }
        },
        "65031d0c97944e04b662668e1ac570d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0830340e22a14b69b13d8b70853cef5f",
            "placeholder": "​",
            "style": "IPY_MODEL_76fbfddf9af344baa30ad1e47c5f5617",
            "value": "Map: 100%"
          }
        },
        "5af85ee86131408bb01b5908b2031b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_660221be809645aca8d17b0ad74571df",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_394cdf5e77814174a958e1368fab372b",
            "value": 11
          }
        },
        "c75edbdac896400291f7b82f44feffc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33431d2c30e94b0bae3de7e1a4446fe1",
            "placeholder": "​",
            "style": "IPY_MODEL_9b2e7d739a6d439f9087643e48ad4003",
            "value": " 11/11 [00:00&lt;00:00, 63.39 examples/s]"
          }
        },
        "7e26880af48d49f9984bdfe429f878e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0830340e22a14b69b13d8b70853cef5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76fbfddf9af344baa30ad1e47c5f5617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "660221be809645aca8d17b0ad74571df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "394cdf5e77814174a958e1368fab372b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33431d2c30e94b0bae3de7e1a4446fe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b2e7d739a6d439f9087643e48ad4003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}